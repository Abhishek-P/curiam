{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agreement Study\n",
    "\n",
    "We randomly selected 3 cases of medium length (70-150 sentences) for double annotation to assess agreement of the revised annotation schema and guidelines. Annotation for this study was conducted by Michael and Nathan.\n",
    "\n",
    "This notebook covers:\n",
    "\n",
    "- Some high level stats about the annotations and disagreements\n",
    "- IAA metrics, including F1 and Gamma\n",
    "- Qualitative analysis of the disagreements\n",
    "\n",
    "Cases for agreement study:\n",
    "\n",
    "- 12625853_mixed_alito\n",
    "- 12628561_ootc_sotomayor\n",
    "- 12625931_dissent_thomas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mkranzlein/michael/dev/curiam\n"
     ]
    }
   ],
   "source": [
    "# Allows for seamless use of updated src\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Switch to top of curiam directory for easier paths\n",
    "%cd ../../..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from curiam import agreement\n",
    "\n",
    "from curiam.inception import tsv_processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_folder = \"data/full_scale/agreement_study\"\n",
    "\n",
    "# These are list of opinions which are list of sentences which are lists of tokens\n",
    "# eg opinions_m[0][0][0] is the 0-th token of the 0-th sentence of the 0-th opinion in the agreement study.\n",
    "opinions = [tsv_processing.process_opinion_file(f\"{agreement_folder}/michael/{filename}\")\n",
    "          for filename in os.listdir(f\"{agreement_folder}/michael\")\n",
    "          if filename.endswith(\".tsv\")]\n",
    "\n",
    "opinions_n = [tsv_processing.process_opinion_file(f\"{agreement_folder}/nathan/{filename}\")\n",
    "          for filename in os.listdir(f\"{agreement_folder}/nathan\")\n",
    "          if filename.endswith(\".tsv\")]\n",
    "\n",
    "# Set 4th column of each token to Nathan's annotation\n",
    "# Each token now has the format: [sentence_num, tok_str, michael_annotation, nathan_annotation]\n",
    "for i, opinion in enumerate(opinions):\n",
    "    for j, sentence in enumerate(opinion):\n",
    "        for k, token in enumerate(sentence):\n",
    "            token.append(opinions_n[i][j][k][2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How many sentences?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(opinion) for opinion in opinions])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How many tokens?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9109\n"
     ]
    }
   ],
   "source": [
    "token_total = sum([len(sentence) for opinion in opinions for sentence in opinion])\n",
    "print(token_total)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many tokens received at least one label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens with at least one label:\n",
      "Michael: 4616 (50.68%)\n",
      "Nathan: 4275 (46.93%)\n"
     ]
    }
   ],
   "source": [
    "def get_token_coverage(sentence, annotation_column):\n",
    "    return sum([1 if token[annotation_column] != \"_\" else 0 for token in sentence])\n",
    "\n",
    "coverage_m = sum([get_token_coverage(sentence, 2) for opinion in opinions for sentence in opinion])\n",
    "coverage_n = sum([get_token_coverage(sentence, 3) for opinion in opinions for sentence in opinion])\n",
    "\n",
    "print(f\"Tokens with at least one label:\")\n",
    "print(f\"Michael: {coverage_m} ({coverage_m/token_total*100:.2f}%)\")\n",
    "print(f\"Nathan: {coverage_n} ({coverage_n/token_total*100:.2f}%)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How many spans did each annotator annotate?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Agreement\n",
    "\n",
    "### Agreement Overall\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygamma_agreement import Continuum\n",
    "from pyannote.core import Segment\n",
    "from pygamma_agreement import CombinedCategoricalDissimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opinion_gamma(opinion):\n",
    "    continuum = Continuum()\n",
    "    offset = 0\n",
    "    for sentence in opinion:\n",
    "        annotations_m = tsv_processing.get_annotations(sentence, annotation_column=2)\n",
    "        annotations_n = tsv_processing.get_annotations(sentence, annotation_column=3)\n",
    "        for annotation in annotations_m:\n",
    "            category, start, end = annotation[0], annotation[1], annotation[2]\n",
    "            continuum.add(\"m\", Segment(start+offset, end+offset+1), category)\n",
    "        for annotation in annotations_n:\n",
    "            category, start, end = annotation[0], annotation[1], annotation[2]\n",
    "            continuum.add(\"n\", Segment(start+offset, end+offset+1), category)\n",
    "        offset += len(sentence)\n",
    "    gamma_list = []\n",
    "    dissim = CombinedCategoricalDissimilarity(alpha=1, beta=1)\n",
    "    gamma_results = continuum.compute_gamma(dissim)\n",
    "    return gamma_results.gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8060260297658612\n",
      "0.8241309632062263\n",
      "0.8666907699474498\n"
     ]
    }
   ],
   "source": [
    "for opinion in opinions:\n",
    "    print(get_opinion_gamma(opinion))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### P, R, F1\n",
    "\n",
    "### Agreement By Category\n",
    "\n",
    "## Qualitative Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curiam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
