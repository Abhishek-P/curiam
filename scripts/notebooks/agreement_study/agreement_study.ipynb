{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agreement Study\n",
    "\n",
    "We randomly selected 3 cases of medium length (70-150 sentences) for double annotation to assess agreement of the revised annotation schema and guidelines. Annotation for this study was conducted by Michael and Nathan.\n",
    "\n",
    "This notebook covers:\n",
    "\n",
    "- Some high level stats about the annotations and disagreements\n",
    "- IAA metrics, including F1 and Gamma\n",
    "- Qualitative analysis of the disagreements\n",
    "\n",
    "Cases for agreement study:\n",
    "\n",
    "- 12625853_mixed_alito\n",
    "- 12628561_ootc_sotomayor\n",
    "- 12625931_dissent_thomas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mkranzlein/michael/dev/curiam\n"
     ]
    }
   ],
   "source": [
    "# Allows for seamless use of updated src\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Switch to top of curiam directory for easier paths\n",
    "%cd ../../..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from curiam import agreement\n",
    "\n",
    "from curiam.inception import tsv_processing\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_folder = \"data/full_scale/agreement_study\"\n",
    "\n",
    "# These are list of opinions which are list of sentences which are lists of tokens\n",
    "# eg opinions_m[0][0][0] is the 0-th token of the 0-th sentence of the 0-th opinion in the agreement study.\n",
    "opinions = [tsv_processing.process_opinion_file(f\"{agreement_folder}/michael/{filename}\")\n",
    "          for filename in os.listdir(f\"{agreement_folder}/michael\")\n",
    "          if filename.endswith(\".tsv\")]\n",
    "\n",
    "opinions_n = [tsv_processing.process_opinion_file(f\"{agreement_folder}/nathan/{filename}\")\n",
    "          for filename in os.listdir(f\"{agreement_folder}/nathan\")\n",
    "          if filename.endswith(\".tsv\")]\n",
    "\n",
    "# Set 4th column of each token to Nathan's annotation\n",
    "# Each token now has the format: [sentence_num, tok_str, michael_annotation, nathan_annotation]\n",
    "for i, opinion in enumerate(opinions):\n",
    "    for j, sentence in enumerate(opinion):\n",
    "        for k, token in enumerate(sentence):\n",
    "            token.append(opinions_n[i][j][k][2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How many sentences?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(opinion) for opinion in opinions])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How many tokens?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9109\n"
     ]
    }
   ],
   "source": [
    "token_total = sum([len(sentence) for opinion in opinions for sentence in opinion])\n",
    "print(token_total)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many tokens received at least one label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens with at least one label:\n",
      "Michael: 4616 (50.68%)\n",
      "Nathan: 4275 (46.93%)\n"
     ]
    }
   ],
   "source": [
    "def get_token_coverage(sentence, annotation_column):\n",
    "    return sum([1 if token[annotation_column] != \"_\" else 0 for token in sentence])\n",
    "\n",
    "coverage_m = sum([get_token_coverage(sentence, 2) for opinion in opinions for sentence in opinion])\n",
    "coverage_n = sum([get_token_coverage(sentence, 3) for opinion in opinions for sentence in opinion])\n",
    "\n",
    "print(f\"Tokens with at least one label:\")\n",
    "print(f\"Michael: {coverage_m} ({coverage_m/token_total*100:.2f}%)\")\n",
    "print(f\"Nathan: {coverage_n} ({coverage_n/token_total*100:.2f}%)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How many spans did each annotator annotate?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Agreement\n",
    "\n",
    "### Agreement Overall\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygamma_agreement import Continuum\n",
    "from pyannote.core import Segment\n",
    "from pygamma_agreement import CombinedCategoricalDissimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opinion_gamma(opinion, excluded_categories=[]):\n",
    "    continuum = Continuum()\n",
    "    offset = 0\n",
    "    for sentence in opinion:\n",
    "        annotations_m = tsv_processing.get_annotations(sentence, annotation_column=2)\n",
    "        annotations_n = tsv_processing.get_annotations(sentence, annotation_column=3)\n",
    "        for annotation in annotations_m:\n",
    "            category, start, end = annotation[0], annotation[1], annotation[2]\n",
    "            if category in excluded_categories:\n",
    "                continue\n",
    "            continuum.add(\"m\", Segment(start+offset, end+offset+1), category)\n",
    "        for annotation in annotations_n:\n",
    "            category, start, end = annotation[0], annotation[1], annotation[2]\n",
    "            if category in excluded_categories:\n",
    "                continue\n",
    "            continuum.add(\"n\", Segment(start+offset, end+offset+1), category)\n",
    "        offset += len(sentence)\n",
    "    dissim = CombinedCategoricalDissimilarity(alpha=1, beta=1)\n",
    "    gamma_results = continuum.compute_gamma(dissim)\n",
    "    return gamma_results.gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8076161318013454\n",
      "0.8247066494288938\n",
      "0.8648076435148637\n"
     ]
    }
   ],
   "source": [
    "for opinion in opinions:\n",
    "    print(get_opinion_gamma(opinion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6944353963751995\n",
      "0.7471369718588388\n",
      "0.7429975028577418\n"
     ]
    }
   ],
   "source": [
    "for opinion in opinions:\n",
    "    print(get_opinion_gamma(opinion, excluded_categories=[\"Direct Quote\", \"Legal Source\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6583071730224289\n",
      "0.6778096615916295\n",
      "0.5092599582426991\n"
     ]
    }
   ],
   "source": [
    "for opinion in opinions:\n",
    "    print(get_opinion_gamma(opinion, excluded_categories=[\"Direct Quote\", \"Legal Source\", \"Metalinguistic Cue\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### P, R, F1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Appeal to Meaning',\n",
    "              'Definition',\n",
    "              'Direct Quote',\n",
    "              'Example Use',\n",
    "              'Focal Term',\n",
    "              'Language Source',\n",
    "              'Legal Source',\n",
    "              'Metalinguistic Cue',\n",
    "              'Named Interpretive Rule']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_categories(label):\n",
    "    token_categories = []\n",
    "    indexed_annotations = label.split(\"|\")\n",
    "    for indexed_annotation in indexed_annotations:\n",
    "        colon_index = indexed_annotation.index(\":\")\n",
    "        category = indexed_annotation[:colon_index]\n",
    "        token_categories.append(category)\n",
    "    return token_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set = set()\n",
    "for opinion in opinions:\n",
    "    for sentence in opinion:\n",
    "        for token in sentence:\n",
    "            if token[2] != \"_\":\n",
    "                label_set.update(set(get_token_categories(token[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_opinion_p_r_f1(opinion):\n",
    "    for category in categories:\n",
    "        token_labels_m = []\n",
    "        token_labels_n = []\n",
    "        for sentence in opinion:\n",
    "            # Create single-class labels for each token for Michael and Nathan\n",
    "            for token in sentence:\n",
    "                label_m = token[2]\n",
    "                label_n = token[3]\n",
    "                if label_m == \"_\":\n",
    "                    token_labels_m.append(0)\n",
    "                \n",
    "                elif category in get_token_categories(label_m):\n",
    "                    token_labels_m.append(1)\n",
    "                else:\n",
    "                    token_labels_m.append(0)\n",
    "                if label_n == \"_\":\n",
    "                    token_labels_n.append(0)\n",
    "                elif category in get_token_categories(label_n):\n",
    "                    token_labels_n.append(1)\n",
    "                else:\n",
    "                    token_labels_n.append(0)\n",
    "        if sum(token_labels_m) > 0 and sum(token_labels_n) > 0:\n",
    "            p, r, f1, _ = precision_recall_fscore_support(token_labels_m, token_labels_n, average=\"macro\")\n",
    "            print(f\"{p:.4f}\\t {r:.4f}\\t {f1:.4f}\\t {category}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6098\t 0.5864\t 0.5956\t Appeal to Meaning\n",
      "0.9126\t 0.9313\t 0.9217\t Definition\n",
      "0.9949\t 0.9743\t 0.9842\t Direct Quote\n",
      "0.9251\t 0.7841\t 0.8377\t Example Use\n",
      "0.9313\t 0.8496\t 0.8860\t Focal Term\n",
      "0.9996\t 0.9756\t 0.9873\t Language Source\n",
      "0.9965\t 0.9982\t 0.9973\t Legal Source\n",
      "0.9198\t 0.8624\t 0.8888\t Metalinguistic Cue\n",
      "0.5708\t 0.6976\t 0.6037\t Named Interpretive Rule\n"
     ]
    }
   ],
   "source": [
    "print_opinion_p_r_f1(opinions[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Agreement By Category\n",
    "\n",
    "## Qualitative Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curiam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
