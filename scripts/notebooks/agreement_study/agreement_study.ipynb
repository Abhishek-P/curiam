{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agreement Study\n",
    "\n",
    "We randomly selected 3 cases of medium length (70-150 sentences) for double annotation to assess agreement of the revised annotation schema and guidelines. Annotation for this study was conducted by Michael and Nathan.\n",
    "\n",
    "This notebook covers:\n",
    "\n",
    "- Some high level stats about the annotations and disagreements\n",
    "- IAA metrics, including F1 and Gamma\n",
    "- Qualitative analysis of the disagreements\n",
    "\n",
    "Cases for agreement study:\n",
    "\n",
    "- 12625853_mixed_alito\n",
    "- 12628561_ootc_sotomayor\n",
    "- 12625931_dissent_thomas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mkranzlein/michael/dev/curiam\n"
     ]
    }
   ],
   "source": [
    "# Allows for seamless use of updated src\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Switch to top of curiam directory for easier paths\n",
    "%cd ../../..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from curiam import agreement\n",
    "\n",
    "from curiam.inception import tsv_processing\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_folder = \"data/full_scale/agreement_study\"\n",
    "\n",
    "# These are list of opinions which are list of sentences which are lists of tokens\n",
    "# eg opinions_m[0][0][0] is the 0-th token of the 0-th sentence of the 0-th opinion in the agreement study.\n",
    "opinions = [tsv_processing.process_opinion_file(f\"{agreement_folder}/michael/{filename}\")\n",
    "          for filename in os.listdir(f\"{agreement_folder}/michael\")\n",
    "          if filename.endswith(\".tsv\")]\n",
    "\n",
    "opinions_n = [tsv_processing.process_opinion_file(f\"{agreement_folder}/nathan/{filename}\")\n",
    "          for filename in os.listdir(f\"{agreement_folder}/nathan\")\n",
    "          if filename.endswith(\".tsv\")]\n",
    "\n",
    "# Set 4th column of each token to Nathan\"s annotation\n",
    "# Each token now has the format: [sentence_num, tok_str, michael_annotation, nathan_annotation]\n",
    "for i, opinion in enumerate(opinions):\n",
    "    for j, sentence in enumerate(opinion):\n",
    "        for k, token in enumerate(sentence):\n",
    "            nathan_label_dict = opinions_n[i][j][k][2]\n",
    "            token.append(nathan_label_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How many sentences?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(opinion) for opinion in opinions])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How many tokens?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9109\n"
     ]
    }
   ],
   "source": [
    "token_total = sum([len(sentence) for opinion in opinions for sentence in opinion])\n",
    "print(token_total)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many tokens received at least one label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens with at least one label:\n",
      "Michael: 4616 (50.68%)\n",
      "Nathan: 4275 (46.93%)\n"
     ]
    }
   ],
   "source": [
    "def get_token_coverage(sentence, annotation_column):\n",
    "    return sum([1 if len(token[annotation_column][\"categories\"]) > 0\n",
    "                else 0 for token in sentence])\n",
    "\n",
    "coverage_m = sum([get_token_coverage(sentence, 2) for opinion in opinions for sentence in opinion])\n",
    "coverage_n = sum([get_token_coverage(sentence, 3) for opinion in opinions for sentence in opinion])\n",
    "\n",
    "print(f\"Tokens with at least one label:\")\n",
    "print(f\"Michael: {coverage_m} ({coverage_m/token_total*100:.2f}%)\")\n",
    "print(f\"Nathan: {coverage_n} ({coverage_n/token_total*100:.2f}%)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How many spans did each annotator annotate?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Agreement\n",
    "\n",
    "### Agreement Overall\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygamma_agreement import Continuum\n",
    "from pyannote.core import Segment\n",
    "from pygamma_agreement import CombinedCategoricalDissimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opinion_gamma(opinion, excluded_categories=[]):\n",
    "    continuum = Continuum()\n",
    "    offset = 0\n",
    "    for sentence in opinion:\n",
    "        annotations_m = tsv_processing.get_sentence_annotations(sentence, annotation_column=2)\n",
    "        annotations_n = tsv_processing.get_sentence_annotations(sentence, annotation_column=3)\n",
    "        for annotation in annotations_m:\n",
    "            category, start, end = annotation[0], annotation[1], annotation[2]\n",
    "            if category in excluded_categories:\n",
    "                continue\n",
    "            continuum.add(\"m\", Segment(start+offset, end+offset+1), category)\n",
    "        for annotation in annotations_n:\n",
    "            category, start, end = annotation[0], annotation[1], annotation[2]\n",
    "            if category in excluded_categories:\n",
    "                continue\n",
    "            continuum.add(\"n\", Segment(start+offset, end+offset+1), category)\n",
    "        offset += len(sentence)\n",
    "    dissim = CombinedCategoricalDissimilarity(alpha=1, beta=1)\n",
    "    gamma_results = continuum.compute_gamma(dissim)\n",
    "    return gamma_results.gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.808, 0.824, 0.864]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinion_gammas = []\n",
    "for opinion in opinions:\n",
    "    opinion_gammas.append(round(get_opinion_gamma(opinion), 3))\n",
    "\n",
    "opinion_gammas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gamma should really be calculated at a document level (which we've done), so to get an overall gamma measurement for the corpus,\n",
    "we can calculate a token-weighted average. That is, average the gamma scores, accounting for the length of each opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_token_counts = [sum([len(sentence) for sentence in opinion]) for opinion in opinions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8291017674827095\n"
     ]
    }
   ],
   "source": [
    "tokens_total = sum(opinion_token_counts)\n",
    "weighted_gammas = []\n",
    "for gamma, token_count in zip(opinion_gammas, opinion_token_counts):\n",
    "    weighted_gammas.append(gamma * (token_count / token_total))\n",
    "weighted_average_gamma = sum(weighted_gammas)\n",
    "print(weighted_average_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6964367061547254\n",
      "0.7448524042406597\n",
      "0.7420227226965087\n"
     ]
    }
   ],
   "source": [
    "for opinion in opinions:\n",
    "    print(get_opinion_gamma(opinion, excluded_categories=[\"Direct Quote\", \"Legal Source\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6536598816466577\n",
      "0.6848057469361514\n",
      "0.500687653017608\n"
     ]
    }
   ],
   "source": [
    "for opinion in opinions:\n",
    "    print(get_opinion_gamma(opinion, excluded_categories=[\"Direct Quote\", \"Legal Source\", \"Metalinguistic Cue\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### P, R, F1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"Appeal to Meaning\",\n",
    "    \"Definition\",\n",
    "    \"Direct Quote\",\n",
    "    \"Example Use\",\n",
    "    \"Focal Term\",\n",
    "    \"Language Source\",\n",
    "    \"Legal Source\",\n",
    "    \"Metalinguistic Cue\",\n",
    "    \"Named Interpretive Rule\"\n",
    "    ]\n",
    "\n",
    "categories_to_abbreviations = {\n",
    "    \"Appeal to Meaning\": \"ATM\",\n",
    "    \"Definition\": \"D\",\n",
    "    \"Direct Quote\": \"DQ\",\n",
    "    \"Example Use\": \"EU\",\n",
    "    \"Focal Term\": \"FT\",\n",
    "    \"Language Source\": \"LaS\",\n",
    "    \"Legal Source\": \"LeS\",\n",
    "    \"Metalinguistic Cue\": \"MC\",\n",
    "    \"Named Interpretive Rule\": \"NIR\"\n",
    "}\n",
    "\n",
    "abbreviations_to_categories = {\n",
    "    \"ATM\": \"Appeal to Meaning\",\n",
    "    \"D\": \"Definition\",\n",
    "    \"DQ\": \"Direct Quote\",\n",
    "    \"EU\": \"Example Use\",\n",
    "    \"FT\": \"Focal Term\",\n",
    "    \"LaS\": \"Language Source\",\n",
    "    \"LeS\": \"Legal Source\",\n",
    "    \"MC\": \"Metalinguistic Cue\",\n",
    "    \"NIR\": \"Named Interpretive Rule\",\n",
    "}\n",
    "\n",
    "# TODO: confirm category order in sec 3 matches this\n",
    "# Categories ordered how they're presented in the paper\n",
    "ordered_categories = [\"FT\", \"D\", \"MC\", \"DQ\", \"LaS\", \"LeS\", \"NIR\", \"EU\", \"ATM\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focal Term & 0.931 & 0.850 & 0.886\\\\\n",
      "Definition & 0.913 & 0.931 & 0.922\\\\\n",
      "Metalinguistic Cue & 0.920 & 0.862 & 0.889\\\\\n",
      "Direct Quote & 0.995 & 0.974 & 0.984\\\\\n",
      "Language Source & 1.000 & 0.976 & 0.987\\\\\n",
      "Legal Source & 0.997 & 0.998 & 0.997\\\\\n",
      "Named Interpretive Rule & 0.571 & 0.698 & 0.604\\\\\n",
      "Example Use & 0.925 & 0.784 & 0.838\\\\\n",
      "Appeal to Meaning & 0.610 & 0.586 & 0.596\\\\\n"
     ]
    }
   ],
   "source": [
    "def print_opinion_p_r_f1(opinion):\n",
    "    for abbreviated_category in ordered_categories:\n",
    "        category = abbreviations_to_categories[abbreviated_category]\n",
    "        token_labels_m = []\n",
    "        token_labels_n = []\n",
    "        for sentence in opinion:\n",
    "            # Create single-class labels for each token for Michael and Nathan\n",
    "            for token in sentence:\n",
    "                label_dict_m = token[2]\n",
    "                label_dict_n = token[3]\n",
    "                if category in label_dict_m[\"categories\"]:\n",
    "                    token_labels_m.append(1)\n",
    "                else:\n",
    "                    token_labels_m.append(0)\n",
    "                if category in label_dict_n[\"categories\"]:\n",
    "                    token_labels_n.append(1)\n",
    "                else:\n",
    "                    token_labels_n.append(0)\n",
    "        if sum(token_labels_m) > 0 and sum(token_labels_n) > 0:\n",
    "            p, r, f1, _ = precision_recall_fscore_support(token_labels_m, token_labels_n, average=\"macro\")\n",
    "            print(f\"{category} & {p:0.3f} & {r:0.3f} & {f1:0.3f}\\\\\\\\\")\n",
    "\n",
    "print_opinion_p_r_f1(opinions[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Span-level Exact Match"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Agreement By Category\n",
    "\n",
    "## Qualitative Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curiam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
