{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agreement Study\n",
    "\n",
    "We randomly selected 3 cases of medium length (70-150 sentences) for double annotation to assess agreement of the revised annotation schema and guidelines. Annotation for this study was conducted by Michael and Nathan.\n",
    "\n",
    "This notebook covers:\n",
    "\n",
    "- Some high level stats about the annotations and disagreements\n",
    "- IAA metrics, including F1 and Gamma\n",
    "- Qualitative analysis of the disagreements\n",
    "\n",
    "Cases for agreement study:\n",
    "\n",
    "- 12625853_mixed_alito\n",
    "- 12628561_ootc_sotomayor\n",
    "- 12625931_dissent_thomas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mkranzlein/michael/dev/curiam\n"
     ]
    }
   ],
   "source": [
    "# Allows for seamless use of updated src\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Switch to top of curiam directory for easier paths\n",
    "%cd ../..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from curiam import categories\n",
    "from curiam.preprocessing import inception_tsv\n",
    "\n",
    "from pyannote.core import Segment\n",
    "from pygamma_agreement import Continuum\n",
    "from pygamma_agreement import CombinedCategoricalDissimilarity\n",
    "from sklearn.metrics import precision_recall_fscore_support\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_path = Path(\"data/full_scale/agreement_study\")\n",
    "\n",
    "# These are list of opinions which are list of sentences which are lists of tokens\n",
    "# eg opinions_m[0][0][0] is the 0-th token of the 0-th sentence of the 0-th opinion in the agreement study.\n",
    "opinions = [inception_tsv.process_opinion_file(opinion_path) for opinion_path\n",
    "            in agreement_path.joinpath(\"michael\").glob(\"*.tsv\")]\n",
    "\n",
    "opinions_n = [inception_tsv.process_opinion_file(opinion_path) for opinion_path\n",
    "              in agreement_path.joinpath(\"nathan\").glob(\"*.tsv\")]\n",
    "\n",
    "# Set 4th column of each token to Nathan's annotation\n",
    "# Each token now has the format: [sentence_num, tok_str, michael_annotation, nathan_annotation]\n",
    "for i, opinion in enumerate(opinions):\n",
    "    for j, sentence in enumerate(opinion):\n",
    "        for k, token in enumerate(sentence):\n",
    "            nathan_label_dict = opinions_n[i][j][k][2]\n",
    "            token.append(nathan_label_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How many sentences?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(opinion) for opinion in opinions])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How many tokens?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9109\n"
     ]
    }
   ],
   "source": [
    "token_total = sum([len(sentence) for opinion in opinions for sentence in opinion])\n",
    "print(token_total)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many tokens received at least one label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens with at least one label:\n",
      "Michael: 4616 (50.68%)\n",
      "Nathan: 4275 (46.93%)\n"
     ]
    }
   ],
   "source": [
    "def get_token_coverage(sentence, annotation_column):\n",
    "    return sum([1 if len(token[annotation_column][\"categories\"]) > 0\n",
    "                else 0 for token in sentence])\n",
    "\n",
    "coverage_m = sum([get_token_coverage(sentence, 2) for opinion in opinions for sentence in opinion])\n",
    "coverage_n = sum([get_token_coverage(sentence, 3) for opinion in opinions for sentence in opinion])\n",
    "\n",
    "print(f\"Tokens with at least one label:\")\n",
    "print(f\"Michael: {coverage_m} ({coverage_m/token_total*100:.2f}%)\")\n",
    "print(f\"Nathan: {coverage_n} ({coverage_n/token_total*100:.2f}%)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How many spans did each annotator annotate?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Agreement\n",
    "\n",
    "### Agreement Overall\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opinion_gamma(opinion, excluded_categories=[]):\n",
    "    continuum = Continuum()\n",
    "    offset = 0\n",
    "    for sentence in opinion:\n",
    "        annotations_m = inception_tsv.get_sentence_annotations(sentence, annotation_column=2)\n",
    "        annotations_n = inception_tsv.get_sentence_annotations(sentence, annotation_column=3)\n",
    "        for annotation in annotations_m:\n",
    "            category, start, end = annotation[0], annotation[1], annotation[2]\n",
    "            if category in excluded_categories:\n",
    "                continue\n",
    "            continuum.add(\"m\", Segment(start+offset, end+offset+1), category)\n",
    "        for annotation in annotations_n:\n",
    "            category, start, end = annotation[0], annotation[1], annotation[2]\n",
    "            if category in excluded_categories:\n",
    "                continue\n",
    "            continuum.add(\"n\", Segment(start+offset, end+offset+1), category)\n",
    "        offset += len(sentence)\n",
    "    dissim = CombinedCategoricalDissimilarity(alpha=1, beta=1)\n",
    "    # .005 is a pretty intense precision value (default is .02)\n",
    "    # Lower is more precise, but more compute-intensive\n",
    "    gamma_results = continuum.compute_gamma(dissim, precision_level=.005)\n",
    "    return gamma_results.gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma for each opinion in agreement study:  [0.808, 0.825, 0.866]\n"
     ]
    }
   ],
   "source": [
    "opinion_gammas = []\n",
    "for opinion in opinions:\n",
    "    opinion_gammas.append(round(get_opinion_gamma(opinion), 3))\n",
    "\n",
    "print(\"Gamma for each opinion in agreement study: \", opinion_gammas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gamma should really be calculated at a document level (which we've done), so to get an overall gamma measurement for the corpus,\n",
    "we can calculate a token-weighted average. That is, average the gamma scores, accounting for the length of each opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-weighted gamma average for whole agreement study: 0.830\n"
     ]
    }
   ],
   "source": [
    "opinion_token_counts = [sum([len(sentence) for sentence in opinion]) for opinion in opinions]\n",
    "\n",
    "tokens_total = sum(opinion_token_counts)\n",
    "weighted_gammas = []\n",
    "for gamma, token_count in zip(opinion_gammas, opinion_token_counts):\n",
    "    weighted_gammas.append(gamma * (token_count / token_total))\n",
    "weighted_average_gamma = sum(weighted_gammas)\n",
    "print(f\"Token-weighted gamma average for whole agreement study: {weighted_average_gamma:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma for each opinion in agreement study:  [0.697, 0.747, 0.743]\n"
     ]
    }
   ],
   "source": [
    "opinion_gammas_no_dq_or_les = []\n",
    "for opinion in opinions:\n",
    "    opinion_gammas_no_dq_or_les.append(round(get_opinion_gamma(opinion, excluded_categories=[\"Direct Quote\", \"Legal Source\"]), 3))\n",
    "\n",
    "print(\"Gamma for each opinion in agreement study: \", opinion_gammas_no_dq_or_les)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-weighted gamma average for whole agreement study: 0.724\n"
     ]
    }
   ],
   "source": [
    "opinion_token_counts = [sum([len(sentence) for sentence in opinion]) for opinion in opinions]\n",
    "\n",
    "tokens_total = sum(opinion_token_counts)\n",
    "weighted_gammas = []\n",
    "for gamma, token_count in zip(opinion_gammas_no_dq_or_les, opinion_token_counts):\n",
    "    weighted_gammas.append(gamma * (token_count / token_total))\n",
    "weighted_average_gamma = sum(weighted_gammas)\n",
    "print(f\"Token-weighted gamma average for whole agreement study: {weighted_average_gamma:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### P, R, F1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table saved to results/tables/agreement_study_p_r_f1.txt\n"
     ]
    }
   ],
   "source": [
    "table_output = \"\"\n",
    "for category in categories.ORDERED_CATEGORIES:\n",
    "    token_labels_m = []\n",
    "    token_labels_n = []\n",
    "    for opinion in opinions:\n",
    "        for sentence in opinion:\n",
    "            # Create single-class labels for each token for Michael and Nathan\n",
    "            for token in sentence:\n",
    "                label_dict_m = token[2]\n",
    "                label_dict_n = token[3]\n",
    "                if category in label_dict_m[\"categories\"]:\n",
    "                    token_labels_m.append(1)\n",
    "                else:\n",
    "                    token_labels_m.append(0)\n",
    "                if category in label_dict_n[\"categories\"]:\n",
    "                    token_labels_n.append(1)\n",
    "                else:\n",
    "                    token_labels_n.append(0)\n",
    "    if sum(token_labels_m) > 0 and sum(token_labels_n) > 0:\n",
    "        p, r, f1, _ = precision_recall_fscore_support(token_labels_m, token_labels_n, average=\"macro\")\n",
    "        table_output+=(f\"{category} & {p:0.3f} & {r:0.3f} & {f1:0.3f}\\\\\\\\\\n\")\n",
    "output_path = Path(\"results/tables/agreement_study_p_r_f1.txt\")\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(table_output)\n",
    "print(f\"Table saved to {output_path.as_posix()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Span-level Exact Match\n",
    "\n",
    "For each of Michael's annotations, did Nathan have an identical span?\n",
    "\n",
    "If we treat Michael's annotations as gold, `precision = matches / nathan_count` and `recall = matches / michael_count`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Agreement By Category\n",
    "\n",
    "## Qualitative Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curiam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
