{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agreement Study\n",
    "\n",
    "We randomly selected 3 cases of medium length (70-150 sentences) for double annotation to assess agreement of the revised annotation schema and guidelines. Annotation for this study was conducted by Michael and Nathan.\n",
    "\n",
    "This notebook covers:\n",
    "\n",
    "- Some high level stats about the annotations and disagreements\n",
    "- IAA metrics, including F1 and Gamma\n",
    "- Qualitative analysis of the disagreements\n",
    "\n",
    "Cases for agreement study:\n",
    "\n",
    "- 12625853_mixed_alito\n",
    "- 12628561_ootc_sotomayor\n",
    "- 12625931_dissent_thomas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd -q ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from pyannote.core import Segment\n",
    "from pygamma_agreement import CombinedCategoricalDissimilarity, Continuum\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from curiam import categories\n",
    "from curiam.document import Sentence\n",
    "from curiam.preprocessing import inception_tsv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_path = Path(\"data/main/agreement_study\")\n",
    "\n",
    "# These are list of opinions which are list of sentences which are lists of tokens\n",
    "# eg opinions_m[0][0][0] is the 0-th token of the 0-th sentence of the 0-th opinion in the agreement study.\n",
    "opinions_m = [inception_tsv.process_opinion_file(opinion_path,opinion_path.name)\n",
    "            for opinion_path in agreement_path.joinpath(\"michael\").glob(\"*.tsv\")]\n",
    "\n",
    "opinions_n = [inception_tsv.process_opinion_file(opinion_path, opinion_path.name)\n",
    "              for opinion_path in agreement_path.joinpath(\"nathan\").glob(\"*.tsv\")]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How many sentences?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(opinion) for opinion in opinions_m])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How many tokens?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9109\n"
     ]
    }
   ],
   "source": [
    "token_total = sum([len(sentence) for opinion in opinions_m for sentence in opinion])\n",
    "print(token_total)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many tokens received at least one label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens with at least one label:\n",
      "Michael: 4616 (50.68%)\n",
      "Nathan: 4275 (46.93%)\n"
     ]
    }
   ],
   "source": [
    "def get_token_coverage(sentence: Sentence):\n",
    "    return sum([1 if token.annotations is not None\n",
    "                else 0 for token in sentence])\n",
    "\n",
    "coverage_m = sum([get_token_coverage(sentence) for opinion in opinions_m for sentence in opinion])\n",
    "coverage_n = sum([get_token_coverage(sentence) for opinion in opinions_n for sentence in opinion])\n",
    "\n",
    "print(\"Tokens with at least one label:\")\n",
    "print(f\"Michael: {coverage_m} ({coverage_m/token_total*100:.2f}%)\")\n",
    "print(f\"Nathan: {coverage_n} ({coverage_n/token_total*100:.2f}%)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How many spans did each annotator annotate?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Agreement\n",
    "\n",
    "### Agreement Overall\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Annotation(category='Legal Source', start=0, end=1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinions_m[0].sentences[4].get_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence(id=4, tokens=[Token(id=0, text='Ibid', annotations=[TokenAnnotation(id=0, category='Legal Source')]), Token(id=1, text='.', annotations=[TokenAnnotation(id=0, category='Legal Source')])])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinions_m[0].sentences[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opinion_gamma(opinion_m, opinion_n, excluded_categories=[]):\n",
    "    continuum = Continuum()\n",
    "    offset = 0\n",
    "    for sentence_m, sentence_n in zip(opinion_m, opinion_n):\n",
    "        annotations_m = sentence_m.get_annotations()\n",
    "        annotations_n = sentence_n.get_annotations()\n",
    "        for annotation in annotations_m:\n",
    "            category = annotation.category\n",
    "            start = annotation.start\n",
    "            end = annotation.end\n",
    "            if category in excluded_categories:\n",
    "                continue\n",
    "            continuum.add(\"m\", Segment(start+offset, end+offset+1), category)\n",
    "        for annotation in annotations_n:\n",
    "            category = annotation.category\n",
    "            start = annotation.start\n",
    "            end = annotation.end\n",
    "            if category in excluded_categories:\n",
    "                continue\n",
    "            continuum.add(\"n\", Segment(start+offset, end+offset+1), category)\n",
    "        offset += len(sentence_m)\n",
    "    dissim = CombinedCategoricalDissimilarity(alpha=1, beta=1)\n",
    "    # .005 is a pretty intense precision value (default is .02)\n",
    "    # Lower is more precise, but more compute-intensive\n",
    "    gamma_results = continuum.compute_gamma(dissim, precision_level=.005)\n",
    "    return gamma_results.gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma for each opinion in agreement study:  [0.80818, 0.82429, 0.86519]\n"
     ]
    }
   ],
   "source": [
    "opinion_gammas = []\n",
    "for opinion_m, opinion_n in zip(opinions_m, opinions_n):\n",
    "    opinion_gammas.append(round(get_opinion_gamma(opinion_m, opinion_n), 5))\n",
    "\n",
    "print(\"Gamma for each opinion in agreement study: \", opinion_gammas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gamma should really be calculated at a document level (which we've done), so to get an overall gamma measurement for the corpus,\n",
    "we can calculate a token-weighted average. That is, average the gamma scores, accounting for the length of each opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-weighted gamma average for whole agreement study: 0.830\n"
     ]
    }
   ],
   "source": [
    "opinion_token_counts = [sum([len(sentence) for sentence in opinion]) for opinion in opinions_m]\n",
    "\n",
    "tokens_total = sum(opinion_token_counts)\n",
    "weighted_gammas = []\n",
    "for gamma, token_count in zip(opinion_gammas, opinion_token_counts):\n",
    "    weighted_gammas.append(gamma * (token_count / token_total))\n",
    "weighted_average_gamma = sum(weighted_gammas)\n",
    "print(f\"Token-weighted gamma average for whole agreement study: {weighted_average_gamma:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma for each opinion in agreement study:  [0.695, 0.745, 0.742]\n"
     ]
    }
   ],
   "source": [
    "gammas_no_dq_or_les = []\n",
    "for opinion_m, opinion_n in zip(opinions_m, opinions_n):\n",
    "    gammas_no_dq_or_les.append(round(get_opinion_gamma(opinion_m, opinion_n,\n",
    "                                                       excluded_categories=[\"Direct Quote\", \"Legal Source\"]), 3))\n",
    "\n",
    "print(\"Gamma for each opinion in agreement study: \", gammas_no_dq_or_les)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-weighted gamma average for whole agreement study: 0.723\n"
     ]
    }
   ],
   "source": [
    "opinion_token_counts = [sum([len(sentence) for sentence in opinion]) for opinion in opinions_m]\n",
    "\n",
    "tokens_total = sum(opinion_token_counts)\n",
    "weighted_gammas = []\n",
    "for gamma, token_count in zip(gammas_no_dq_or_les, opinion_token_counts):\n",
    "    weighted_gammas.append(gamma * (token_count / token_total))\n",
    "weighted_average_gamma = sum(weighted_gammas)\n",
    "print(f\"Token-weighted gamma average for whole agreement study: {weighted_average_gamma:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### P, R, F1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table saved to results/tables/agreement_study_p_r_f1.txt\n"
     ]
    }
   ],
   "source": [
    "table_output = \"\"\n",
    "for category in categories.ORDERED_CATEGORIES:\n",
    "    token_labels_m = []\n",
    "    token_labels_n = []\n",
    "    for opinion in opinions_m:\n",
    "        for sentence in opinion:\n",
    "            # Create single-class labels for each token for Michael and Nathan\n",
    "            for token in sentence:\n",
    "                if category in token.get_categories():\n",
    "                    token_labels_m.append(1)\n",
    "                else:\n",
    "                    token_labels_m.append(0)\n",
    "    for opinion in opinions_n:\n",
    "        for sentence in opinion:\n",
    "            # Create single-class labels for each token for Michael and Nathan\n",
    "            for token in sentence:\n",
    "                if category in token.get_categories():\n",
    "                    token_labels_n.append(1)\n",
    "                else:\n",
    "                    token_labels_n.append(0)\n",
    "    if sum(token_labels_m) > 0 and sum(token_labels_n) > 0:\n",
    "        p, r, f1, _ = precision_recall_fscore_support(token_labels_m, token_labels_n, average=\"macro\")\n",
    "        table_output+=(f\"{category} & {p:0.3f} & {r:0.3f} & {f1:0.3f}\\\\\\\\\\n\")\n",
    "output_path = Path(\"results/tables/agreement_study_p_r_f1.txt\")\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(table_output)\n",
    "print(f\"Table saved to {output_path.as_posix()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Span-level Exact Match\n",
    "\n",
    "For each of Michael's annotations, did Nathan have an identical span?\n",
    "\n",
    "If we treat Michael's annotations as gold, `precision = matches / nathan_count` and `recall = matches / michael_count`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Agreement By Category\n",
    "\n",
    "## Qualitative Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curiam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
