{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mkranzlein/michael/dev/curiam\n"
     ]
    }
   ],
   "source": [
    "# Allows for seamless use of updated src\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Switch to top of curiam directory for easier paths\n",
    "%cd ../../\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from curiam import categories\n",
    "from curiam.inception import tsv_processing\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table: total number of opinions and number of majority, concurring, and dissenting opinions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table saved to results/tables/opinion_characteristics.txt\n"
     ]
    }
   ],
   "source": [
    "opinion_counts = {}\n",
    "\n",
    "# These are plain text files that whose names have been standarized.\n",
    "# TODO: Fix filenames in INCEpTION so exported filenames can be used directly\n",
    "opinions_with_docket_numbers_dir = Path(\n",
    "    \"data/full_scale/processed/inception_files_with_docket_numbers/\")\n",
    "\n",
    "for opinion_path in opinions_with_docket_numbers_dir.glob(\"*.txt\"):\n",
    "    # First 2 pieces make up docket number (e.g. 17_834), which we don't need\n",
    "    _, _, opinion_type, author = opinion_path.name.split(\"_\")\n",
    "    # Reformat authors for displaying in table\n",
    "    author = author[:-4]\n",
    "    author = author[0].upper() + author[1:]\n",
    "    if author not in opinion_counts.keys():\n",
    "        opinion_counts[author] = Counter()\n",
    "    opinion_counts[author][opinion_type] += 1\n",
    "\n",
    "authors = sorted(opinion_counts.keys())\n",
    "total = 0\n",
    "majority_total = 0\n",
    "concurrence_total = 0\n",
    "dissent_total = 0\n",
    "table_output = \"\"\n",
    "for author in authors:\n",
    "    # Calculate row sums and increment column totals\n",
    "    majority = opinion_counts[author][\"ootc\"]\n",
    "    majority_total += majority\n",
    "    concurrence = opinion_counts[author][\"concurrence\"]\n",
    "    concurrence_total += concurrence\n",
    "    dissent = opinion_counts[author][\"dissent\"]\n",
    "    dissent_total += dissent\n",
    "    # Total for this particular justice\n",
    "    justice_total = (majority + concurrence + dissent)\n",
    "    total += justice_total\n",
    "    table_output += f\"{author} & {majority} & {concurrence} & {dissent} & {justice_total}\\\\\\\\\\n\"\n",
    "table_output += \"\\\\bottomrule\\n\"\n",
    "table_output += f\"& {majority_total} & {concurrence_total} & {dissent_total} & {total}\\\\\\\\\"\n",
    "output_path = Path(\"results/tables/opinion_characteristics.txt\")\n",
    "with output_path.open(\"w\") as f:\n",
    "    f.write(table_output)\n",
    "print(f\"Table saved to {output_path.as_posix()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: token 'accord' has label * and note: Guidelines: this should not be annotated\\; it's a term of art\n"
     ]
    }
   ],
   "source": [
    "opinions_dir = Path(\"data/full_scale/annotated\")\n",
    "\n",
    "# These are lists of opinions which are lists of sentences\n",
    "# which are lists of tokens.\n",
    "# E.g. opinions_m[0][0][0] is the 0-th token of the 0-th sentence of the \n",
    "# 0-th opinion in the agreement study.\n",
    "opinions = [tsv_processing.process_opinion_file(opinion_path)\n",
    "            for opinion_path in sorted(opinions_dir.glob(\"*.tsv\"), key= lambda path: path.name)]\n",
    "\n",
    "assert len(opinions) == 41"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many tokens are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179690"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_total = sum([len(sentence) for opinion in opinions for sentence in opinion])\n",
    "token_total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many sentences are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7068"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(opinion) for opinion in opinions])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Metalinguistic Cue', 8, 8],\n",
       " ['Definition', 9, 32],\n",
       " ['Direct Quote', 24, 27],\n",
       " ['Focal Term', 25, 26]]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All of the annotations in the corpus, grouped by sentence\n",
    "all_annotations = [tsv_processing.get_sentence_annotations(sentence)\n",
    "               for opinion in opinions for sentence in opinion]\n",
    "\n",
    "# Remove sentences which have 0 annotations, which will show as empty lists\n",
    "all_annotations = [x for x in all_annotations if len(x) > 0]\n",
    "\n",
    "# Annotations for the first sentence that has any annotations\n",
    "all_annotations[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many annotations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9820"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(sentence_annotations) for sentence_annotations in all_annotations])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many sentences with at least one annotation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4447"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_annotations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What percentage of tokens are covered by at least one annotation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens annotated with at least one category: 68718 (38.24%)\n"
     ]
    }
   ],
   "source": [
    "def get_token_coverage(sentence, annotation_column=2):\n",
    "    return sum([1 if len(token[annotation_column][\"categories\"]) > 0\n",
    "                else 0 for token in sentence])\n",
    "\n",
    "coverage = sum([get_token_coverage(sentence, 2) for opinion in opinions for sentence in opinion])\n",
    "\n",
    "print(f\"Tokens annotated with at least one category: {coverage} ({(coverage/token_total) * 100:.2f}%)\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table: number of annotation for each category and their average length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_freqs_and_lens = {}\n",
    "for sentence_annotations in all_annotations:\n",
    "    for annotated_span in sentence_annotations:\n",
    "        category, start, end = [val for val in annotated_span]\n",
    "        category_freqs_and_lens.setdefault(category, {\"count\": 0, \"lengths\": [] })\n",
    "        category_freqs_and_lens[category][\"count\"] += 1\n",
    "        category_freqs_and_lens[category][\"lengths\"].append((end - start) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focal Term & 1043 & 2.5{ (1.8)}\\\\\n",
      "Definition & 273 & 12.2{ (9.4)}\\\\\n",
      "Metalinguistic Cue & 1784 & 1.3{ (0.7)}\\\\\n",
      "Direct Quote & 2577 & 10.9{ (10.1)}\\\\\n",
      "Language Source & 74 & 10.0{ (4.3)}\\\\\n",
      "Legal Source & 3706 & 8.6{ (8.2)}\\\\\n",
      "Named Interpretive Rule & 51 & 5.1{ (7.1)}\\\\\n",
      "Example Use & 115 & 23.5{ (12.5)}\\\\\n",
      "Appeal to Meaning & 196 & 27.8{ (13.0)}\\\\\n"
     ]
    }
   ],
   "source": [
    "for category in categories.ORDERED_CATEGORIES:\n",
    "    frequency = category_freqs_and_lens[category][\"count\"]\n",
    "    mean = statistics.mean(category_freqs_and_lens[category][\"lengths\"])\n",
    "    st_dev = statistics.stdev(category_freqs_and_lens[category][\"lengths\"])\n",
    "    print(f\"{category} & {frequency} & {mean:.1f}{'{'} ({st_dev:.1f}){'}'}\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scatter plot opinion length vs coverage percentage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curiam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
