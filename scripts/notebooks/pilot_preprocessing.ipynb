{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%cd ../.."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pilot Annotation Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the opinions for this project come from the Harvard Caselaw Access Project. The HTML was preprocessed using BeautifulSoup, cleaned, and then imported into UBIAI for annotation. \n",
    "\n",
    "\n",
    "During the pilot annotation, to increase opinion coverage, only the first 2000 tokens of each opinion (give or take) were annotated.\n",
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tokenizers import pre_tokenizers\n",
    "from tokenizers.pre_tokenizers import Whitespace, Punctuation, Digits\n",
    "\n",
    "from curiam.preprocessing import cap_parsing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Case Selection\n",
    "\n",
    "18 cases from the 2019 Supreme Court term were selected for annotation. 41 opinions are associated with these 18 cases, but for reasons discussed later on, only 32 were included in the pilot phase of annotation. The cases are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_names = [\n",
    "    \"Rotkiske v. Klemm\",\n",
    "    \"Peter v. NantKwest\",\n",
    "    \"Shular v. United States\",\n",
    "    \"Intel Corp. Investment Policy Committee v. Sulyma\",\n",
    "    \"Kansas v. Garcia\",\n",
    "    \"Comcast v. National Association of African-American-Owned Media\",\n",
    "    \"Babb v. Wilkie\",\n",
    "    \"Atlantic Richfield Co. v. Christian\",\n",
    "    \"Thryv, Inc. v. Click-To-Call Technologies, LP\",\n",
    "    \"County of Maui v. Hawaii Wildlife Fund, No. 18-260\",\n",
    "    \"Romag Fasteners, Inc. v. Fossil, Inc.\",\n",
    "    \"Barton v. Barr\",\n",
    "    \"Banister v. Davis\",\n",
    "    \"Nasrallah v. Barr\",\n",
    "    \"Lomax v. Ortiz-Marquez\",\n",
    "    \"United States Forest Service v. Cowpasture River Preservation Assn.\",\n",
    "    \"Bostock v. Clayton County\",\n",
    "    \"Liu v. Securities and Exchange Commission\"\n",
    "]\n",
    "\n",
    "case_docket_numbers = [\n",
    "    \"18-328\",\n",
    "    \"18-801\",\n",
    "    \"18-6662\",\n",
    "    \"18-1116\",\n",
    "    \"17-834\",\n",
    "    \"18-1171\",\n",
    "    \"18-882\",\n",
    "    \"17-1498\",\n",
    "    \"18-916\",\n",
    "    \"18-260\",\n",
    "    \"18-1233\",\n",
    "    \"18-725\",\n",
    "    \"18-6943\",\n",
    "    \"18-1432\",\n",
    "    \"18-8369\",\n",
    "    \"18-1584\",\n",
    "    \"17-1618\",\n",
    "    \"18-1501\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download opinion HTML from Harvard Caselaw Access Project (CAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = []\n",
    "for docket_number in case_docket_numbers:\n",
    "    potentials = cap_parsing.get_case_by_docket_number(docket_number)\n",
    "    case_id = cap_parsing.get_longest_casebody_in_list(potentials)\n",
    "    case_json = cap_parsing.get_case_by_id(case_id)\n",
    "    cases.append(case_json)\n",
    "\n",
    "# Get some metadata, parse html, prettify it, and save to file.\n",
    "os.makedirs(\"data/pilot/raw\", exist_ok=True)\n",
    "for case_json in cases:\n",
    "    case_html = case_json[\"casebody\"][\"data\"]\n",
    "    opinion_start = case_html.find('article class=\"opinion\"')\n",
    "    first_opinion_p = case_html[opinion_start:].find(\"<p\")\n",
    "    last_closing_p_index = case_html[::-1].find(\">p/\")\n",
    "    character_index_before_last_closing_p = len(case_html) - last_closing_p_index - 4\n",
    "    opinion_html = case_html[opinion_start + first_opinion_p:character_index_before_last_closing_p]\n",
    "    pretty_html = BeautifulSoup(opinion_html, \"html.parser\").prettify()\n",
    "\n",
    "    with open(f\"data/pilot/raw/{case_json['id']}.txt\", \"w\") as f:\n",
    "        f.write(pretty_html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Segmentation\n",
    "At this point, we have 18 .txt files containing HTML. Each file represents one case, but cases can have 1 opinion or multiple opinions (opinion of the court, concurrences, dissents). Using regex patterns related to the words `dissent` and `concur`, a triple line break was manually added between the end of one opinion and the beginning of a new one.\n",
    "\n",
    "This was done in copies of the 18 files located in [data/pilot/processed/sentence_segmented_html](../../data/pilot/processed/sentence_segmented_html).\n",
    "\n",
    "Several opinion breaks were missed at this step, which is why the pilot annotation only included 32 cases instead of 41. We caught this error for the full corpus and manually checked each case against a third-party website (https://oyez.org) to confirm we had the full number of opinions for each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_by_filename = {}\n",
    "\n",
    "segmented_html_path = \"data/processed/sentence_segmented_html\"\n",
    "\n",
    "for filename in os.listdir(segmented_html_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(f\"{segmented_html_path}/{filename}\", \"r\") as f:\n",
    "            text = f.read()\n",
    "            html_opinions = text.split(\"\\n\\n\\n\")\n",
    "            parsed_opinions = []\n",
    "            for opinion_html in html_opinions:\n",
    "                opinion_paragraphs = cap_parsing.parse_opinion_html(opinion_html)\n",
    "                parsed_opinions.append(opinion_paragraphs)\n",
    "            opinions_by_filename[filename] = parsed_opinions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and conversion to TSV for UBIAI import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_to_docket_numbers = {}\n",
    "for filename in opinions_by_filename.keys():\n",
    "    filenames_to_docket_numbers[filename] = cap_parsing.get_filename_from_docket_number(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_tag_paragraph(pre_tokenized_text):\n",
    "    tsv_output = \"\"\n",
    "    for token, (token_start, token_stop) in pre_tokenized_text:\n",
    "        tsv_output += f\"{token}\\tO\\n\"\n",
    "    tsv_output += \"\\n\"\n",
    "    return tsv_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat tokens into TSV style from https://ubiai.tools/Docs and convert preannotations to BIO style\n",
    "pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Punctuation(), Digits()])\n",
    "\n",
    "case_tsv_path = \"data/pilot/processed/case_tsv\"\n",
    "os.makedirs(case_tsv_path, exist_ok=True)\n",
    "\n",
    "for filename, opinions in opinions_by_filename.items():\n",
    "    docket_number = filenames_to_docket_numbers[filename]\n",
    "    for opinion_number, opinion in enumerate(opinions):\n",
    "        opinion_tsv = \"-DOCSTART- -X- O O\\n\"\n",
    "        for paragraph in opinion:\n",
    "            paragraph_text = paragraph[\"paragraph_text\"]\n",
    "            citations = list(paragraph[\"citations\"].keys())\n",
    "            pre_tokenized_text = pre_tokenizer.pre_tokenize_str(paragraph_text)\n",
    "            opinion_tsv += bio_tag_paragraph(pre_tokenized_text)\n",
    "        with open(f\"{case_tsv_path}/{docket_number}_opinion_{opinion_number:02d}.tsv\", \"w\") as f:\n",
    "            f.write(opinion_tsv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curiam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
